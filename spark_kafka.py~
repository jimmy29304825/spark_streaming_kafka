# load model
from pyspark.mllib.recommendation import ALS
from pyspark.mllib.recommendation import MatrixFactorizationModel
import math, time, json
from kafka import KafkaConsumer
import redis
from pymongo import MongoClient
import pyodbc

client = MongoClient(host="10.120.28.5", port=27017)
db = client['music']
music = db.music
r = redis.Redis(host='10.120.28.22', port=6379)

sc = SparkContext()
model_path = "/home/cloudera/Desktop/datasets/music/music_lens_als"
same_model = MatrixFactorizationModel.load(sc, model_path)
# load data
complete_ratings_raw_data = sc.textFile("/home/cloudera/Desktop/datasets/music/ff2_data.csv")
# set column names
complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]
# transfer data
## 1. filiter: remove first row
## 2. map: split data by ","
## 3. map: get first 3 columns
complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header).map(lambda line: line.split(",")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()
print("There are %s recommendations in the complete dataset" % (complete_ratings_data.count()))

seed = 5
iterations = 10
regularization_parameter = 0.1
ranks = [4, 8, 12]
errors = [0, 0, 0]
err = 0
tolerance = 0.02
min_error = float('inf')
best_rank = 4
best_iteration = -1

# load music neta data
complete_music_raw_data = sc.textFile("/home/cloudera/Desktop/datasets/music/songs_metadata_file_new.csv")
complete_music_raw_data_header = complete_music_raw_data.take(1)[0]
complete_music_data = complete_music_raw_data.filter(lambda line: line!=complete_music_raw_data_header).map(lambda line: line.split(",")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2],tokens[3])).cache()
complete_music_titles = complete_music_data.map(lambda x: (int(x[0]),x[1], x[2]))
print("There are %s musics in the complete dataset" % (complete_music_titles.count()))

# join data's def
def get_counts_and_averages(ID_and_ratings_tuple):
    nratings = len(ID_and_ratings_tuple[1])
    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)

# group by every mucis couts data by music id
music_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())
# get average and counts for each music 
music_ID_with_avg_ratings_RDD = music_ID_with_ratings_RDD.map(get_counts_and_averages)
# get counts for each music 
music_rating_counts_RDD = music_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))

    
# 設定要連線到Kafka集群的相關設定, 產生一個Kafka的Consumer的實例
consumer = KafkaConsumer(
    # 指定Kafka集群伺服器
    bootstrap_servers=["3.112.123.88:9092"],
    value_deserializer=lambda m: json.loads(m.decode('ascii')),
    auto_offset_reset="latest"
)

# 讓Consumer向Kafka集群訂閱指定的topic
consumer.subscribe(topics="music_test1")
new_user_ratings = []
# 持續的拉取Kafka有進來的訊息
print("Now listening for incoming messages ...")
# 持續監控是否有新的record進來
for record in consumer:
    msgValue = eval(record.value) 
    new_user_ID = msgValue['userid'] 
    song_index = msgValue['music'][0] 
    song_id = msgValue['music'][1] 
    rating = msgValue['rating'] 
    print(new_user_ID, song_id, song_index, rating) 
    data = (new_user_ID, song_id, rating) 
    new_user_ratings.append(data) 
    cnxn=pyodbc.connect("DSN=SpliceODBC64") 
    cursor=cnxn.cursor()
    sql = "insert into longdb.music_rating values (" + str(new_user_ID) + ", " + str(song_id) + ", " + str(rating) + ")"
    cursor.execute(sql) 
    result_l = cnxn.commit()
    cursor.close()
    cnxn.close() 

    if song_index != 'm3': 
        continue 
    else: 
        print(new_user_ratings)
        new_user_ratings_RDD = sc.parallelize(new_user_ratings)
        
        print('New user ratings: %s' % new_user_ratings_RDD.take(3))
        # merge new data into old data
        complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)
        # train model again with new data
        from time import time
        t0 = time()
        new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)
        tt = time() - t0
        print("New model trained in %s seconds" % round(tt,3))
    
        new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just music IDs
        # keep just those not on the ID list
        new_user_unrated_music_RDD = (complete_music_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))
        # Use the input RDD, new_user_unrated_music_RDD, with new_ratings_model.predictAll() to predict new ratings for the musics
        new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_music_RDD)
    
        # get every predicct result for new user
        new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))
        # merge data with music info
        new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_music_titles).join(music_rating_counts_RDD)
        new_user_recommendations_rating_title_and_count_RDD.take(3)
        # transfer data format
        new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))
        # sort data by rating score and list first 25 data
        top_musics = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])
        print('TOP recommended musics (with more than 25 reviews):\n%s' % '\n'.join(map(str, top_musics)))
        result_r = r.hset('music', new_user_ID, str(top_musics))
        j = {'user': new_user_ID, 'music': top_musics}
        result_m = music.insert_one(j)
        new_user_ratings = []

        
        print(result_l, result_r, result_m, new_user_ratings)
        

        


